{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader # dataset을 순회가능한 객체(iterable)\n",
    "from torchvision import datasets # 샘플(data) and labels\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공개 데이터 셋에서 학습 데이터 내려받기\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root ='data',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = ToTensor(),    \n",
    ")\n",
    "\n",
    "# 공개 데이터 셋에서 테스트 데이터 내려받기\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root ='data',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor(),    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset을 DataLoader의 인자로 전달 <br>\n",
    "이는 데이터셋을 순회 가능한 객체(iterable)로 감싸고, 자동화된 배치(batch), 샘플링(sampling), 섞기(shuffle) 및 다중 프로세스로 데이터 불러오기(multiprocess data loading)를 지원  <br>\n",
    "즉, 데이터로더 객체의 각 요소는 64개의 feature와 label을 묶음(batch)으로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W] : torch.Size([64, 1, 28, 28])\n",
      "Shape of y : torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_dataloader = DataLoader(training_data, batch_size = batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size = batch_size)\n",
    "\n",
    "for X, y in test_dataloader :\n",
    "    print(\"Shape of X [N, C, H, W] : {}\".format(X.shape))\n",
    "    print(\"Shape of y : {}\".format(y.shape))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 만들기! <BR>\n",
    "파이토치에서 신경망 모델은 nn.Module을 상속받는 클래스를 생성하여 정의 <br>\n",
    "__init__ 함수에서 신경망의 계층(layer)들을 정의하고 forward 함수에서 신경망에 데이터를 어떻게 전달할지 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Module을 상속한 subclass가 신경망 모델로 사용되기 위해선 앞서 소개한 두 메소드를 override 해야한다. <br>\n",
    "\n",
    "__init__(self): initialize; 내가 사용하고 싶은, 내 신경망 모델에 사용될 구성품들을 정의 및 초기화 하는 메소드이다. <br>\n",
    "forward(self, x): specify the connections;  이닛에서 정의된 구성품들을 연결하는 메소드이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<flatten 예시> <br>\n",
    "input_image = torch.rand(64, 28, 28)   # torch.Size([64,28,28])<br>\n",
    "flatten = nn.Flatten()<br>\n",
    "flat_image = flatten(input_image)  # torch.Size([64, 784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 학습에 사용할 CPU나 GPU 장치 얻기\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "# 모델 저의하기\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # 부모 클래스인 nn.Module을 super()을 사용해서 초기화\n",
    "        self.flatten = nn.Flatten() # 연속된 범위를 가진 차원을 하나의 텐서로 Flatten해준다.\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logists = self.linear_relu_stack(x)\n",
    "        return logists\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 매개변수 최적화하기 <br>\n",
    "모델을 학습하려면 loss function과 옵티마이저가 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 학습 단계(training loop)에서 모델은 (배치로 제공되는) 학습 데이터셋에 대한 예측을 수행하고, 예측 오류를 역전파하여 모델의 매개 변수를 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch에서는 왜 항상 optimizer.zero_grad()를 해줄까? <br>\n",
    "\n",
    "Pytorch에서는 gradients값들을 추후에 backward를 해줄때 계속 더해주기 때문\"에 우리는 항상 backpropagation을 하기전에 gradients를 zero로 만들어주고 시작을 해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 함수는 nn.Module 바깥에 지정해주는구나\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 예측 오류 계산\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() # 모델을 평가 모드로 설정\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad(): #  안에서 계산된 그래디언트가 모델 가중치에 업데이트되지 않도록 합니다. 이것은 테스트 중에는 그래디언트가 필요하지 않기 때문\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()                            # item() 함수는 PyTorch 텐서(tensor)에서 스칼라 값을 추출하는 데 사용되는 메서드\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item() # pred.argmax(1) 가장 큰 값을 반환 \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(size)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.298333  [   64/60000]\n",
      "loss: 2.285150  [ 6464/60000]\n",
      "loss: 2.267569  [12864/60000]\n",
      "loss: 2.262990  [19264/60000]\n",
      "loss: 2.247089  [25664/60000]\n",
      "loss: 2.220218  [32064/60000]\n",
      "loss: 2.233292  [38464/60000]\n",
      "loss: 2.201061  [44864/60000]\n",
      "loss: 2.195114  [51264/60000]\n",
      "loss: 2.163738  [57664/60000]\n",
      "10000\n",
      "Test Error: \n",
      " Accuracy: 45.4%, Avg loss: 2.150543 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.161657  [   64/60000]\n",
      "loss: 2.148637  [ 6464/60000]\n",
      "loss: 2.090363  [12864/60000]\n",
      "loss: 2.105748  [19264/60000]\n",
      "loss: 2.050162  [25664/60000]\n",
      "loss: 1.998369  [32064/60000]\n",
      "loss: 2.029078  [38464/60000]\n",
      "loss: 1.953416  [44864/60000]\n",
      "loss: 1.956926  [51264/60000]\n",
      "loss: 1.883521  [57664/60000]\n",
      "10000\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 1.874129 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.908821  [   64/60000]\n",
      "loss: 1.876827  [ 6464/60000]\n",
      "loss: 1.753479  [12864/60000]\n",
      "loss: 1.795454  [19264/60000]\n",
      "loss: 1.680299  [25664/60000]\n",
      "loss: 1.641784  [32064/60000]\n",
      "loss: 1.666304  [38464/60000]\n",
      "loss: 1.574530  [44864/60000]\n",
      "loss: 1.593043  [51264/60000]\n",
      "loss: 1.486503  [57664/60000]\n",
      "10000\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 1.499391 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.565368  [   64/60000]\n",
      "loss: 1.533144  [ 6464/60000]\n",
      "loss: 1.373972  [12864/60000]\n",
      "loss: 1.447638  [19264/60000]\n",
      "loss: 1.327434  [25664/60000]\n",
      "loss: 1.329876  [32064/60000]\n",
      "loss: 1.348130  [38464/60000]\n",
      "loss: 1.282617  [44864/60000]\n",
      "loss: 1.308580  [51264/60000]\n",
      "loss: 1.207646  [57664/60000]\n",
      "10000\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 1.232027 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.303745  [   64/60000]\n",
      "loss: 1.293135  [ 6464/60000]\n",
      "loss: 1.118505  [12864/60000]\n",
      "loss: 1.224036  [19264/60000]\n",
      "loss: 1.099675  [25664/60000]\n",
      "loss: 1.128667  [32064/60000]\n",
      "loss: 1.154624  [38464/60000]\n",
      "loss: 1.103184  [44864/60000]\n",
      "loss: 1.133437  [51264/60000]\n",
      "loss: 1.046112  [57664/60000]\n",
      "10000\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.067562 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
